{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "collapsed": true,
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport glob\nimport logging\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom imageio import imwrite\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom scipy import ndimage\nimport skimage.morphology\nfrom sklearn.model_selection import KFold, train_test_split\nfrom skimage import io, transform\nfrom skimage.filters import threshold_otsu\n\nimport torch\nimport torchvision\n\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\n#\nimport imgaug\nfrom imgaug import augmenters as iaa\nimport random\nimport skimage\n#\n\nclass Config():\n    if torch.cuda.is_available():\n        ROOT_FOLDER = '/content/.kaggle/competitions/data-science-bowl-2018/'\n    else:\n        ROOT_FOLDER = '/home/bilal/.kaggle/competitions/data-science-bowl-2018/'\n\n    STAGE='1'\n\n    IMGS_FOLDER = 'stage1_train'\n    TARGETS_FOLDER = 'stage1_targets'\n\n    SUBSET = False\n\n    SHUFFLE = True\n    BATCH_SIZE = 4\n    NUM_WORKERS = 3\n\n    KFOLDS = 6\n    PATIENCE = 0\n    EPOCHS = 10\n    LR = 0.0001\n    WEIGHTS = ''\n    \n    RANDOMCROP = 256\n    FLIPLR = 0.5\n    FLIPUD = 0.5\n    ROTATE = 25\n\nconfig = Config()\n\nclass EarlyStopping:\n    def __init__(self):\n        \n        self.best_score = 1e10\n        self.best_epoch = 0 \n\n    def evaluate(self, model, loss, epoch, patience=0):\n        if loss < self.best_score:\n            logging.info('Val score has improved, saving model')\n            self.best_score = loss\n            self.best_epoch = epoch\n            return 'save'\n        elif epoch - self.best_epoch > patience:\n            logging.info('Val score hasn\\'t improved for ' + str(epoch - self.best_epoch) + ' epochs, stopping training')\n            return 'stop'\n        else:\n            logging.info('Val score hasn\\'t improved for ' + str(epoch - self.best_epoch) + ' epochs, not saving model')\n            return 'continue'\n    \ndef calculate_losses(total_train_loss, total_val_loss, train_ids, val_ids, epoch):\n    train_loss = total_train_loss / (len(train_ids) / config.BATCH_SIZE)\n    val_loss = total_val_loss / (len(val_ids) / config.BATCH_SIZE)\n\n    message = 'Epoch # ' + str(epoch) + ' | Training Loss: ' + str(round(train_loss, 4)) + ' | Validation Loss: ' + str(round(val_loss, 4))\n    \n    return message, train_loss, val_loss\n\ndef calculate_kfolds_losses(total_kfolds_train_loss, total_kfolds_val_loss, kfolds, epochs):\n    train_loss = total_kfolds_train_loss / (kfolds * epochs)\n    val_loss = total_kfolds_val_loss / (kfolds * epochs)\n\n    message = '\\nTotal loss over ' + str(kfolds) + ' kfolds and ' + str(epochs) + ' epochs | Training Loss: ' + str(round(train_loss, 4)) + ' | Validation Loss: ' + str(round(val_loss, 4))\n    return message\n\ndef save_model(model, kfold):\n    torch.save(model.state_dict(), './model-kfold-' + str(kfold) + '-best.pt')\n\ndef load_model(model, path):\n    logging.info('Loading saved model')\n    model.load_state_dict(torch.load(path))\n\n    return model\n\ndef get_kfolds(kfolds):\n    if config.SUBSET:\n        ids = glob.glob(os.path.join(config.ROOT_FOLDER, 'stage' + config.STAGE + '_train', '*'))[:20]\n    else:\n        ids = glob.glob(os.path.join(config.ROOT_FOLDER, 'stage' + config.STAGE + '_train', '*'))\n        \n\n    ids = [id.split('/')[-1] for id in ids]\n\n    if kfolds == 1:\n        train_ids, val_ids = train_test_split(ids, test_size=0.1)\n        return [[train_ids, val_ids]]\n    else:\n        kf = KFold(n_splits=kfolds)\n\n        kfolds = []\n        for x, y in kf.split(ids):\n            x = ids[x[0]: x[-1]]\n            y = ids[y[0]: y[-1]]\n\n            kfolds.append([x, y])\n\n        return kfolds\n\ndef get_path(id):\n    img_path = os.path.join(config.ROOT_FOLDER, 'stage' + config.STAGE + '_train', id, 'images', id + '.png')\n    target_path = os.path.join('./targets', id + '.png')\n\n    return img_path, target_path\n\ndef get_edges(img):\n    img = skimage.morphology.binary_dilation(img, selem=np.ones((5,5))).astype(np.uint8)\n    return img\n \n#\ndef get_sizes(mask_folder):\n    mask = glob.glob(os.path.join(mask_folder, 'masks/*'))[0]\n    mask = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n \n    return mask.shape\n\ndef create_masks(root_folder, stage_number, stage_section, output_folder, mode, subset=False):\n    stage_folder = os.path.join(root_folder, 'stage' + stage_number + '_' + stage_section) \n#     os.makedirs(stage_folder + '_' + mode, exist_ok=True)\n    os.makedirs('./' + mode, exist_ok=True)\n\n    if subset:\n        masks_folder = glob.glob(os.path.join(stage_folder, '*'))[:20]\n    else:\n        masks_folder = glob.glob(os.path.join(stage_folder, '*'))        \n    \n    for mask_folder in tqdm(masks_folder):\n        mask_id = mask_folder.split('/')[-1]\n\n        size = get_sizes(mask_folder)\n        masks = np.zeros(size)\n        masks_with_edges = np.zeros(size)\n\n        for mask in glob.glob(os.path.join(mask_folder, 'masks/*')):\n            ###\n            img = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n            img = img / 255.0\n\n            img_with_edges = get_edges(img)\n            \n            masks = np.add(masks, img)\n            masks_with_edges = np.add(masks_with_edges, img_with_edges)\n        \n        target = np.zeros((size[0], size[1], 3))\n        \n        target[:,:,0] = masks == 1\n        target[:,:,1] = masks_with_edges == 2\n        target[:,:,2] = masks == 0\n        \n        target *= 255\n        target = target.astype(np.uint8)\n\n        output_path = os.path.join('./' + mode, mask_id + '.png')        \n        imwrite(output_path, target)\n        \nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, pooling=True):\n        super(ConvBlock, self).__init__()\n        self.pooling = pooling\n      \n        self.conv = nn.Sequential(\n          nn.Conv2d(in_ch, out_ch, 3, padding=1),\n          nn.BatchNorm2d(out_ch),\n          nn.ReLU(inplace=True),\n          nn.Dropout2d(0.2),\n          nn.Conv2d(out_ch, out_ch, 3, padding=1),\n          nn.BatchNorm2d(out_ch),\n          nn.ReLU(inplace=True),\n          nn.Dropout2d(0.2)\n        )\n        \n    def forward(self, x):\n        x = self.conv(x)      \n  \n        if self.pooling == True:\n            x = F.max_pool2d(x, 2)\n                            \n        return x\n\nclass Upsample(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(Upsample, self).__init__()\n\n        self.upsample = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n        self.conv = ConvBlock(in_ch, out_ch, pooling=False)\n\n    def forward(self, x1, x2):\n        upsample = self.upsample(x1)\n        \n        cat = torch.cat([x2, upsample], dim=1)\n        conv = self.conv(cat)\n\n        return conv\n\nclass OutConv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(OutConv, self).__init__()\n\n        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = F.sigmoid(x)\n\n        return x\n\nclass Unet(nn.Module):\n    def __init__(self):\n        super(Unet, self).__init__()\n\n        self.in_conv = ConvBlock(3, 64, pooling=False)\n        self.down_1 = ConvBlock(64, 128)\n        self.down_2 = ConvBlock(128, 256)\n        self.down_3 = ConvBlock(256, 512)\n        self.down_4 = ConvBlock(512, 1024)\n        self.up_1 = Upsample(1024, 512)\n        self.up_2 = Upsample(512, 256)\n        self.up_3 = Upsample(256, 128)\n        self.up_4 = Upsample(128, 64)\n        \n        self.out_conv = OutConv(64, 3)\n\n    def forward(self, x):\n        x = x / 255\n    \n        x1 = self.in_conv(x)\n        x2 = self.down_1(x1)\n        x3 = self.down_2(x2)\n        x4 = self.down_3(x3)\n        x5 = self.down_4(x4)\n        x = self.up_1(x5, x4)\n        x = self.up_2(x, x3)\n        x = self.up_3(x, x2)\n        x = self.up_4(x, x1)\n        \n        outputs = self.out_conv(x)\n\n        return outputs\n\nclass Resize(object):\n    def __call__(self, sample, size):\n        img, mask = sample[0], sample[1]\n\n        img = skimage.transform.resize(img, (size, size), mode='reflect', preserve_range=True)\n        mask = skimage.transform.resize(mask, (size, size), mode='reflect', preserve_range=True)\n\n        img = img.astype(np.uint8)\n        mask = mask.astype(np.uint8)\n\n        return img, mask\n        \nclass RandomCrop(object):\n    def __call__(self, sample, size):\n        img = sample[0]\n        mask = sample[1]\n\n        h, w = img.shape[:2]\n\n        if h != size:\n            top = np.random.randint(0, h - size)\n            left = np.random.randint(0, w - size)\n        else:\n            top = 0\n            left = 0\n\n        img = img[top: top + size, left: left + size]\n        mask = mask[top: top + size, left: left + size]\n\n        return img, mask\n    \n# class CLAHE(object):\n#     def __call__(self, sample):\n#         img, mask = sample[0], sample[1]\n\n#         img = img.reshape(256, 256, 3)\n#         img = img[:,:,[2,1,0]] # flip r and b\n#         img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n#         clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(grid_size,grid_size))\n#         img[:,:,0] = clahe.apply(img[:,:,0])\n#         img = cv2.cvtColor(img, cv2.COLOR_LAB2BGR)\n#         img = img[:,:,[2,1,0]]\n#         img = img.reshape(3, 256, 256)\n        \n#         return img, mask\n        \nclass RescalingIntensity(object):\n    def __call__(self, sample):\n        img, mask = sample[0], sample[1]\n        \n        img_gray = img[:,:,0]\n        if np.mean(img_gray) > 127:\n            img = 255 - img\n    \n        return img, mask\n\nclass FlipLR(object):\n    def __call__(self, sample, p):\n        img, mask = sample[0], sample[1]\n        \n        if random.random() < p:\n            img = img[:, ::-1].copy()\n            mask = mask[:, ::-1].copy()\n            return img, mask\n        return img, mask\n\nclass FlipUD(object):\n    def __call__(self, sample, p):\n        img, mask = sample[0], sample[1]\n        \n        if random.random() < p:\n            img = img[::-1].copy()\n            mask = mask[::-1].copy()\n            return img, mask\n        return img, mask\n\nclass Rotate(object):\n    def __call__(self, sample, max_angle):\n        img, mask = sample[0], sample[1]\n\n        angle = random.randint(0, max_angle)\n        \n        img = skimage.transform.rotate(img, angle, preserve_range=True)\n        mask = skimage.transform.rotate(mask, angle, preserve_range=True)\n        \n        img = img.astype(np.uint8)\n        mask = mask.astype(np.uint8)\n\n        return img, mask\n\nclass ToTensor(object):\n    def __call__(self, sample):\n        img, mask = sample[0], sample[1]\n        \n        img = transforms.ToTensor()(img)\n        mask = transforms.ToTensor()(mask)\n        \n        return img, mask\n    \nclass CLAHE(object):\n    def __call__(self, sample):\n        img, mask = sample[0], sample[1]\n\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2Lab)\n        clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8, 8))\n        img[:, :, 0] = clahe.apply(img[:, :, 0])\n        img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n        \n        return img, mask\n\ndef augmentation(img, mask):\n    randomCrop = RandomCrop()\n    flipLR = FlipLR()\n    flipUD = FlipUD()\n    rotate = Rotate()\n    \n    img, mask = randomCrop([img, mask], config.RANDOMCROP)\n    img, mask = flipLR([img, mask], config.FLIPLR)\n    img, mask = flipUD([img, mask], config.FLIPUD)\n    img, mask = rotate([img, mask], config.ROTATE)\n\n    return img, mask\n\nclass TrainDataset(Dataset):\n    def __init__(self, ids, augmentation=None):\n        self.ids = ids\n        self.augmentation = augmentation\n        \n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        id = self.ids[idx]\n\n        img_path, targets_path = get_path(id)\n        \n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        mask = cv2.imread(targets_path, cv2.IMREAD_COLOR)\n        \n#         img, target = self.augmentation(img, target)\n        \n        clahe = CLAHE()\n        rescalingIntensity = RescalingIntensity()\n        toTensor = ToTensor()\n        randomCrop = RandomCrop()\n        \n#         img, mask = clahe([img, mask])\n        img, mask = rescalingIntensity([img, mask])\n        img, mask = rescalingIntensity([img, mask])\n        img, mask = randomCrop([img, mask], config.RANDOMCROP)\n        img, mask = toTensor([img, mask])\n\n        return {'img': img, 'target': mask}\n    \n# Taken from Heng Cher Keng's April 27 code\nclass WeightedBCELoss2d(nn.Module):\n    def __init__(self):\n        super(WeightedBCELoss2d, self).__init__()\n\n    def forward(self, logits, labels, weights):\n        w = weights.view(-1)\n        z = logits.contiguous().view(-1)\n        t = labels.contiguous().view(-1)\n\n        loss = w*z.clamp(min=0) - w*z*t + w*torch.log(1 + torch.exp(-z.abs()))\n        loss = loss.sum()/(w.sum()+ 1e-12)\n        return loss\n\ndef make_weight(labels_truth):\n    B,C,H,W = labels_truth.size()\n    weight = torch.FloatTensor(B*C*H*W).requires_grad_().cuda()\n\n    pos = labels_truth.detach().sum()\n    neg = B*C*H*W - pos\n    \n    if pos>0:\n        pos_weight = 0.5/pos\n        neg_weight = 0.5/neg\n    else:\n        pos_weight = 0\n        neg_weight = 0\n\n    weight[labels_truth.contiguous().view(-1)> 0.5] = pos_weight\n    weight[labels_truth.contiguous().view(-1)<=0.5] = neg_weight\n\n    weight = weight.view(B,C,H,W)\n    return weight\n\ndef loss(inputs, targets):\n    epsilon = 1e-5\n    inputs = torch.clamp(inputs.cpu(), epsilon, 1. - epsilon)\n    weight = 30 * targets[:,0:1].cpu() + 3 * targets[:,1:2].cpu() + 1 * targets[:,2:3].cpu()\n    \n    loss = - torch.sum(targets.cpu() * weight.cpu() * torch.log(inputs.cpu()) + (1 - targets.cpu()) * torch.log(1 - inputs.cpu()), 1)\n  \n    return loss\n\n    # mask_weights = make_weight(targets[:,0:1])\n    # edges_weights = make_weight(targets[:,1:2])\n    # backgrounds_weights = make_weight(targets[:,2:3])\n\n    # mask_loss = WeightedBCELoss2d()(inputs[:,0:1], targets[:,0:1], mask_weights)\n    # edges_loss = WeightedBCELoss2d()(inputs[:,1:2], targets[:,1:2], edges_weights)\n    # backgrounds_loss = WeightedBCELoss2d()(inputs[:,2:3], targets[:,2:3], backgrounds_weights)\n\n    # loss = 3 * dice_loss(inputs[:,0:1], targets[:,0:1]) + 30 * dice_loss(inputs[:,1:2], targets[:,1:2]) + 1 * dice_loss(inputs[:,2:3], targets[:,2:3])\n\n    return loss\n\ndef dice_loss(inputs, targets):\n    num = targets.size(0)\n    m1  = inputs.view(num,-1)\n    m2  = targets.view(num,-1)\n    intersection = (m1 * m2)\n    score = 2. * (intersection.sum(1)+1) / (m1.sum(1) + m2.sum(1)+1)\n    score = 1 - score.sum()/num\n    return score\n\ndef iou(predict, label):\n\n    # Precision helper function\n    def compute_precision(threshold, iou):\n        matches = iou > threshold\n        true_positives  = np.sum(matches, axis=1) == 1  # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    num_label   = len(np.unique(label  ))\n    num_predict = len(np.unique(predict))\n\n    # Compute intersection between all objects\n    intersection = np.histogram2d(label.flatten(), predict.flatten(), bins=(num_label, num_predict))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(label,   bins = num_label  )[0]\n    area_pred = np.histogram(predict, bins = num_predict)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred,  0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    precision = []\n    average_precision = 0\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = compute_precision(t, iou)\n        p = tp / (tp + fp + fn)\n        precision.append((t, p, tp, fp, fn))\n        average_precision += p\n\n    average_precision /= len(precision)\n    return average_precision, precision\n\ndef show_images(weights):\n    logging.info('Visualizing model')\n    model = Unet()\n    model.load_state_dict(torch.load(weights))\n    model.eval()\n\n    model.cuda()\n  \n    kfolds = get_kfolds(2)\n  \n#     dataset = TrainDataset(kfolds[0][0], x_transform=x_transforms, target_transforms=target_transforms)\n    dataset = TrainDataset(kfolds[0][0], augmentation=augmentation)\n    dataLoader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=config.SHUFFLE, num_workers=config.NUM_WORKERS)\n\n    with torch.no_grad():\n        for data in dataLoader:\n            img, target = data['img'], data['target']\n\n            x = Variable(img).cuda()\n            y = Variable(target).cuda()\n\n            outs = model(x)\n            break\n\n    y = y.detach().cpu().numpy()\n    outs = outs.detach().cpu().numpy()\n\n    fig = plt.figure(figsize=(30, 20))\n\n    ax = plt.subplot(4, 4, 1)\n    ax.set_title('Ground truth')\n    ax.imshow(y[1].reshape(256, 256, 3))\n\n    ax = plt.subplot(4, 4, 2)\n    ax.set_title('Ground truth mask')\n    ax.imshow(y[1][0].reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 3)\n    ax.set_title('Ground truth edges')\n    ax.imshow(y[1][1].reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 4)\n    ax.set_title('Ground truth background')\n    ax.imshow(y[1][2].reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 5)\n    ax.set_title('Ground truth mask - edges')\n    ax.imshow((y[1][0] - y[1][1]).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 6)\n    ax.set_title('Ground truth background - mask')\n    ax.imshow((y[1][2] - y[1][0]).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 7)\n    ax.set_title('Predicted mask')\n    ax.imshow((outs[1][0]).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 8)\n    ax.set_title('Predicted mask with Otsu thresholding')\n    ax.imshow((outs[1][0] > threshold_otsu(outs[1][0])).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 9)\n    ax.set_title('Predicted edges')\n    ax.imshow((outs[1][1]).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 10)\n    ax.set_title('Predicted edges with Otsu thresholding')\n    ax.imshow((outs[1][1] > threshold_otsu(outs[1][1])).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 11)\n    ax.set_title('Predicted background')\n    ax.imshow((outs[1][2]).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 12)\n    ax.set_title('Predicted background with Otsu thresholding')\n    ax.imshow((outs[1][2] > threshold_otsu(outs[1][2])).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 13)\n    ax.set_title('Predicted mask - edges')\n    ax.imshow((outs[1][0]) - (outs[1][1]).reshape(256, 256))  \n\n    ax = plt.subplot(4, 4, 14)\n    ax.set_title('Predicted mask - edges with Otsu thresholding')\n    ax.imshow(((outs[1][0] - outs[1][1]) > threshold_otsu((outs[1][0]) - (outs[1][1]))).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 15)\n    ax.set_title('Predicted background - mask with Otsu thresholding')\n    ax.imshow(((outs[1][2] - outs[1][0]) > threshold_otsu((outs[1][2]) - (outs[1][0]))).reshape(256, 256))",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9555a580-047d-4adf-926d-72da9e3bdf96",
        "_uuid": "822c2519353125180be44907c65e55c16489c44b",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport logging\nfrom tqdm import tqdm\nfrom glob import glob\nimport argparse\n\nimport torch\nimport torchvision\n\nfrom torch.utils.data import DataLoader\n\nimport torch.nn as nn\nfrom torch.autograd import Variable\n    \ndef subset(subset):\n    if subset == 'True':\n        logging.info('Using a subset')\n        config.SUBSET = True\n    else:\n        logging.info('Using the full dataset')\n        config.SUBSET = False\n\ndef preprocess():\n    logging.info('Starting Preprocessing')\n    logging.info('Creating targets')\n    create_masks(config.ROOT_FOLDER, config.STAGE, 'train', config.TARGETS_FOLDER, 'targets', config.SUBSET)\n\ndef train(epochs, weights, kfolds):\n    logging.info('Starting Training')\n    logging.info('Training for ' + str(epochs) + ' epochs')\n\n    kfolds = get_kfolds(kfolds)\n    logging.info(str(len(kfolds)) + ' kfolds in cross validation')\n\n    if weights != '':\n        model = Unet()\n        model = load_model(model, weights)\n\n    total_kfolds_train_loss = 0\n    total_kfolds_val_loss = 0\n\n    for i, kfold in enumerate(kfolds):\n        print('\\n')\n        logging.info('=' * 50)\n        logging.info('Kfold # ' + str(i + 1))\n\n        train_ids, val_ids = kfold[0], kfold[1]\n\n        logging.info('Creating Dataset')\n#         train = TrainDataset(train_ids, x_transform=x_transforms, target_transforms=target_transforms)\n        train = TrainDataset(train_ids, augmentation=augmentation)\n        trainDataloader = DataLoader(train, batch_size=config.BATCH_SIZE, shuffle=config.SHUFFLE, num_workers=config.NUM_WORKERS)\n        val = TrainDataset(val_ids, augmentation=augmentation)\n        valDataloader = DataLoader(val, batch_size=config.BATCH_SIZE, shuffle=config.SHUFFLE, num_workers=config.NUM_WORKERS)\n\n        if weights != '' and i == 0:\n            model = model\n            weights = ''\n        else:\n            model = Unet()\n\n        model.cuda()\n\n        optimizer = optim.Adam(model.parameters(), lr=config.LR)\n\n        early_stopping = EarlyStopping()\n\n        for epoch in range(epochs):\n            epoch += 1\n            print('\\n')\n            logging.info('-' * 50)\n            logging.info('Epoch # ' + str(epoch))\n            \n            total_train_loss = 0\n            for data in tqdm(trainDataloader):\n                img, target = data['img'], data['target']\n\n#                 x = img.requires_grad_().cuda()\n#                 y = target.requires_grad_().cuda()\n                x = Variable(img).cuda()\n                y = Variable(target).cuda()\n\n                optimizer.zero_grad()\n\n                outs = model(x)\n                train_loss = loss(outs, y)\n#                 total_train_loss += (torch.sum(train_loss.view(-1)) / len(train_loss.view(-1))).item()\n                total_train_loss += (torch.sum(train_loss.view(-1)) / len(train_loss.view(-1))).data[0]\n\n                train_loss.backward(gradient=train_loss)\n                optimizer.step()\n\n            total_val_loss = 0\n#             with torch.no_grad():\n            for data in tqdm(valDataloader):\n                img, target = data['img'], data['target']\n\n                x = Variable(img).cuda()\n                y = Variable(target).cuda()\n\n                optimizer.zero_grad()\n\n                outs = model(x)\n                val_loss = loss(outs, y)\n#                     total_val_loss += (torch.sum(val_loss.view(-1)) / len(val_loss.view(-1))).item()\n                total_val_loss += (torch.sum(val_loss.view(-1)) / len(val_loss.view(-1))).data[0]\n\n            message, train_loss, val_loss = calculate_losses(total_train_loss, total_val_loss, train_ids, val_ids, epoch)\n            print(message)\n\n            total_kfolds_train_loss += train_loss\n            total_kfolds_val_loss += val_loss\n\n            action = early_stopping.evaluate(model, val_loss, epoch, config.PATIENCE)\n\n            if action == 'save':\n                save_model(model, i)\n            elif action == 'stop':\n                break\n            else:\n                continue\n    \n    message = calculate_kfolds_losses(total_kfolds_train_loss, total_kfolds_val_loss, config.KFOLDS, config.EPOCHS)\n    print(message)\n\ndef visualize(weights):\n    show_images(weights)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4ddbcb48-0fd4-447a-ab2b-4eed925d2eb3",
        "_uuid": "2115c9b481d47b5fb73237c4dc47abe0bfa86b25",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import glob\nconfig.ROOT_FOLDER = '../input/'\nconfig.SUBSET = True",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "bfcfe20f-d846-45fd-99c1-e26f4323113d",
        "_uuid": "2406c5ff19d5aec40d2f5bd820d9873b37e390fc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_edges(img):\n    img = skimage.morphology.binary_dilation(img, selem=np.ones((3,3))).astype(np.uint8)\n    return img\n \ndef get_sizes(mask_folder):\n    mask = glob.glob(os.path.join(mask_folder, 'masks/*'))[0]\n    mask = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n \n    return mask.shape\n\ndef create_masks(root_folder, stage_number, stage_section, output_folder, mode, subset=False):\n    stage_folder = os.path.join(root_folder, 'stage' + stage_number + '_' + stage_section) \n#     os.makedirs(stage_folder + '_' + mode, exist_ok=True)\n    os.makedirs('./' + mode, exist_ok=True)\n\n    if subset:\n        masks_folder = glob.glob(os.path.join(stage_folder, '*'))[:20]\n    else:\n        masks_folder = glob.glob(os.path.join(stage_folder, '*'))        \n    \n    for mask_folder in tqdm(masks_folder):\n        mask_id = mask_folder.split('/')[-1]\n\n        size = get_sizes(mask_folder)\n        masks = np.zeros(size)\n        masks_with_edges = np.zeros(size)\n\n        for mask in glob.glob(os.path.join(mask_folder, 'masks/*')):\n            ###\n            img = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n            img = img / 255.0\n\n            img_with_edges = get_edges(img)\n            \n            masks = np.add(masks, img)\n            masks_with_edges = np.add(masks_with_edges, img_with_edges)\n        \n        target = np.zeros((size[0], size[1], 3))\n        \n#         target[:,:,0] = masks == 1\n#         target[:,:,1] = masks_with_edges == 2\n#         target[:,:,2] = masks == 0\n        target[:,:,0] = masks_with_edges == 1\n        target[:,:,1] = masks_with_edges == 2\n        target[:,:,2] = masks_with_edges == 0\n        \n        target *= 255\n        target = target.astype(np.uint8)\n\n        return target\ntarget = create_masks(config.ROOT_FOLDER, config.STAGE, 'train', config.TARGETS_FOLDER, 'targets', config.SUBSET)",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "  0%|          | 0/20 [00:00<?, ?it/s]\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a5a87d8a97ab5be752b65fe410bae9e070a3895"
      },
      "cell_type": "code",
      "source": "plt.imshow(target[:,:,0], cmap='gray')",
      "execution_count": 11,
      "outputs": [
        {
          "data": {
            "text/plain": "<matplotlib.image.AxesImage at 0x7f5e782299e8>"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAAD8CAYAAAAIRgN/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": "<matplotlib.figure.Figure at 0x7f5e78269668>"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d49d417f0eb4d596eb22e347241c691a45507ad"
      },
      "cell_type": "code",
      "source": "target[:,:,2].mean()",
      "execution_count": 14,
      "outputs": [
        {
          "data": {
            "text/plain": "152.05682373046875"
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "0b9af1c8-3922-4fea-a98e-369e488632c2",
        "_uuid": "53c9b2d1b0df08c4d71f2260d8ed346f4cd62708",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.imshow(target[:,:,0], cmap='gray')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "060e4aa9-44ae-496e-80d0-5c640ab41087",
        "_uuid": "e4a2514d44f1bbd07aa946d7ad8cc7c2618abd6f",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.imshow(target[:,:,1], cmap='gray')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e78ca78b-e90c-4e5f-924b-f89d5b4d1b32",
        "_uuid": "d570f5662c5935c79e2fa7abf60de61028e885bc",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.imshow(target[:,:,2], cmap='gray')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f3fa075b-17bb-49eb-be92-3041f1427242",
        "_uuid": "d0bfdfdafeb8f82e5360c701dcd87c4efe724ff6",
        "collapsed": true,
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import glob\npreprocess()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "da793ff4-dd70-4ea7-9000-312a160e96f9",
        "_uuid": "c3b58b13122056458275949e3c9890eebdf87217",
        "collapsed": true,
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "import glob\nconfig.PATIENCE = 5\nconfig.KFOLDS = 1\nconfig.BATCH_SIZE = 2\ntrain(1, '', 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9d39a590-2c1d-466a-b89d-eca02f946582",
        "_uuid": "ac41d5d2e95076aec6021e3ad428d8323b8d70db",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def postprocess(weights):\n    model = Unet()\n\n    model.load_state_dict(torch.load(weights, map_location=lambda storage, location: storage))\n    model.eval()\n\n    model.cuda()\n  \n    kfolds = get_kfolds(2)\n  \n    dataset = TrainDataset(kfolds[0][0], augmentation=augmentation)\n    dataLoader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS)\n\n    for data in dataLoader:\n        img, target = data['img'], data['target']\n\n        x = Variable(img).cuda()\n        y = Variable(target).cuda()\n\n        outs = model(x)\n#         break\n\n    x = x.data.cpu().numpy()\n    y = y.data.cpu().numpy()\n    outs = outs.data.cpu().numpy()\n\n    return x, y, outs\n  \n# x, y, outs = postprocess('model-45.pt')\nx, y, outs = postprocess('model-kfold-0-best.pt')\n\nfrom skimage.morphology import label, binary_dilation, erosion, binary_closing\nfrom skimage.segmentation import random_walker\n\n# outs[0][1] *= (255.0/outs[0][1].max())\n\nmask = outs[0,0]\ncontour = outs[0,1]\n    \nmask = (mask*255).astype(np.uint8)\ncontour = (contour*255).astype(np.uint8)\n\n_, mask = cv2.threshold(mask, 0, 255, cv2.THRESH_OTSU)\n_, contour = cv2.threshold(contour, 0, 255, cv2.THRESH_OTSU)\n\nsure_foreground = (contour - mask)\nsure_background = erosion(mask)\n\nmask_plus_contour = cv2.add(mask, contour)\nmask_plus_contour = cv2.cvtColor(mask_plus_contour, cv2.COLOR_GRAY2RGB)\n\nunknown = cv2.subtract(sure_background, sure_foreground)\n\n# Marker labelling\noutput = cv2.connectedComponentsWithStats(sure_foreground)\nlabels = output[1]\nstats = output[2]\n# Add one to all labels so that sure background is not 0, 0 is considered unknown by watershed\n# this way, watershed can distinguish unknown from the background\nlabels = labels + 1\nlabels[unknown==255] = 0\n\nlabels = cv2.watershed(mask_plus_contour, labels)\n# labels = random_walker(mask_plus_contour, labels, multichannel=True)   \n\nlabels[labels==-1] = 0\nlabels[labels==1] = 0\nlabels = labels -1\nlabels[labels==-1] = 0\n\nmean = np.mean(stats[1:,cv2.CC_STAT_AREA])\n\nfor i in range(1, labels.max()):\n     if stats[i, cv2.CC_STAT_AREA] > mean*10 or stats[i, cv2.CC_STAT_AREA] < mean/10:\n        labels[labels==i] = 0\n\n        \ndef renumber_labels(label_img):\n    \"\"\" Re-number nuclei in a labeled image so the nuclei numbers are unique and consecutive.\n    \"\"\"\n    new_label = 0\n    for old_label in np.unique(label_img):\n        if not old_label == new_label:\n            label_img[label_img == old_label] = new_label\n        new_label += 1\n  \n    return label_img\n\n\nlabels = renumber_labels(labels)\nimg = np.concatenate((outs[0][0:1].reshape(256, 256, 1), outs[0][1:2].reshape(256, 256, 1), (outs[0][2]).reshape(256, 256, 1)), axis=-1).reshape(256, 256, 3)\ntruth = np.concatenate((y[0][0:1].reshape(256, 256, 1), y[0][1:2].reshape(256, 256, 1), (y[0][2] > 0).astype(np.uint8).reshape(256, 256, 1)), axis=-1).reshape(256, 256, 3)\nx = np.concatenate((x[0][0:1].reshape(256, 256, 1), x[0][1:2].reshape(256, 256, 1), (x[0][2]).reshape(256, 256, 1)), axis=-1).reshape(256, 256, 3)\n\nplt.imshow(x)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c6f4c2c1-e18e-4c58-b3c6-b348323993bf",
        "_uuid": "7e6766f861a2ecfe880d4d3610c74985e54b3d04",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# x = x.reshape(3, 256, 256)\nx = (x*255).astype(np.uint8)\n\nimg_lab = cv2.cvtColor(x, cv2.COLOR_RGB2Lab)\nclahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8, 8))\nimg_lab[:, :, 0] = clahe.apply(img_lab[:, :, 0])\nimg_output = cv2.cvtColor(img_lab, cv2.COLOR_LAB2RGB)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8daae0de-4a7d-4875-9f0d-403199b893cb",
        "_uuid": "40ad6a9ef413482242a79500740c8d8584864daa",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.imshow(img_output)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "78c886fb-4f15-4ed7-ac18-dd529d542abf",
        "_uuid": "3a9f54111a1911d7280b45aec71d05027d2b3a82",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.imshow(truth)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0a210aec-f36f-4a65-b632-a6204557d782",
        "_uuid": "a5e1790a871b5c549facb870c69d27a44f281bf9",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def rgb_clahe(in_rgb_img):\n    grid_size = 8\n#     in_rgb_img = (in_rgb_img*255).astype(np.uint8)\n    bgr = in_rgb_img[:,:,[2,1,0]] # flip r and b\n    lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(grid_size,grid_size))\n    lab[:,:,0] = clahe.apply(lab[:,:,0])\n    bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n    return bgr[:,:,[2,1,0]]\n\nplt.imshow(rgb_clahe(x))\n# x = x.reshape(256, 256, 3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "939eb072-fa61-4347-92bb-6ebe744c3188",
        "_uuid": "cf44c7f24ef3580ec1eef637bc4174421fc266bd",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "x.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "47f3d5ab-6202-46db-95cf-9ad46adcc9a6",
        "_uuid": "f61bb54304b3af879e72dfd2280760f5a8932cdf",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.imshow(x.reshape(256, 256, 3))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4deec5a0-0574-4c32-8a0f-e4684f3d85d9",
        "_uuid": "44670d5b9413b3c357f579cb9e94096809dcf01a",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# x = x.astype(np.uint8)\nskimage.exposure.equalize_adapthist(x[:,:,0])\n# x[:,:,0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8a1195d0-28a2-43b2-9f84-95de1cb0aab3",
        "_uuid": "74f8d99653a6c41ad0b41991e3cd89fe292a419f",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "!rm -rf ./targets",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}