{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"import os\nimport glob\nimport logging\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom imageio import imwrite\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom scipy import ndimage\nimport skimage.morphology\nfrom sklearn.model_selection import KFold, train_test_split\nfrom skimage import io, transform\nfrom skimage.filters import threshold_otsu\n\nimport torch\nimport torchvision\n\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\n#\nimport imgaug\nfrom imgaug import augmenters as iaa\nimport random\nimport skimage\n#\n\nclass Config():\n    if torch.cuda.is_available():\n        ROOT_FOLDER = '/content/.kaggle/competitions/data-science-bowl-2018/'\n    else:\n        ROOT_FOLDER = '/home/bilal/.kaggle/competitions/data-science-bowl-2018/'\n\n    STAGE='1'\n\n    IMGS_FOLDER = 'stage1_train'\n    TARGETS_FOLDER = 'stage1_targets'\n\n    SUBSET = False\n\n    SHUFFLE = True\n    BATCH_SIZE = 4\n    NUM_WORKERS = 3\n\n    KFOLDS = 6\n    PATIENCE = 0\n    EPOCHS = 10\n    LR = 0.0001\n    WEIGHTS = ''\n    \n    RANDOMCROP = 256\n    FLIPLR = 0.5\n    FLIPUD = 0.5\n    ROTATE = 25\n\nconfig = Config()\n\nclass EarlyStopping:\n    def __init__(self):\n        \n        self.best_score = 1e10\n        self.best_epoch = 0 \n\n    def evaluate(self, model, loss, epoch, patience=0):\n        if loss < self.best_score:\n            logging.info('Val score has improved, saving model')\n            self.best_score = loss\n            self.best_epoch = epoch\n            return 'save'\n        elif epoch - self.best_epoch > patience:\n            logging.info('Val score hasn\\'t improved for ' + str(epoch - self.best_epoch) + ' epochs, stopping training')\n            return 'stop'\n        else:\n            logging.info('Val score hasn\\'t improved for ' + str(epoch - self.best_epoch) + ' epochs, not saving model')\n            return 'continue'\n    \ndef calculate_losses(total_train_loss, total_val_loss, train_ids, val_ids, epoch):\n    train_loss = total_train_loss / (len(train_ids) / config.BATCH_SIZE)\n    val_loss = total_val_loss / (len(val_ids) / config.BATCH_SIZE)\n\n    message = 'Epoch # ' + str(epoch) + ' | Training Loss: ' + str(round(train_loss, 4)) + ' | Validation Loss: ' + str(round(val_loss, 4))\n    \n    return message, train_loss, val_loss\n\ndef calculate_kfolds_losses(total_kfolds_train_loss, total_kfolds_val_loss, kfolds, epochs):\n    train_loss = total_kfolds_train_loss / (kfolds * epochs)\n    val_loss = total_kfolds_val_loss / (kfolds * epochs)\n\n    message = '\\nTotal loss over ' + str(kfolds) + ' kfolds and ' + str(epochs) + ' epochs | Training Loss: ' + str(round(train_loss, 4)) + ' | Validation Loss: ' + str(round(val_loss, 4))\n    return message\n\ndef save_model(model, kfold):\n    torch.save(model.state_dict(), './model-kfold-' + str(kfold) + '-best.pt')\n\ndef load_model(model, path):\n    logging.info('Loading saved model')\n    model.load_state_dict(torch.load(path))\n\n    return model\n\ndef get_kfolds(kfolds):\n    if config.SUBSET:\n        ids = glob.glob(os.path.join(config.ROOT_FOLDER, 'stage' + config.STAGE + '_train', '*'))[:20]\n    else:\n        ids = glob.glob(os.path.join(config.ROOT_FOLDER, 'stage' + config.STAGE + '_train', '*'))\n        \n\n    ids = [id.split('/')[-1] for id in ids]\n\n    if kfolds == 1:\n        train_ids, val_ids = train_test_split(ids, test_size=0.1)\n        return [[train_ids, val_ids]]\n    else:\n        kf = KFold(n_splits=kfolds)\n\n        kfolds = []\n        for x, y in kf.split(ids):\n            x = ids[x[0]: x[-1]]\n            y = ids[y[0]: y[-1]]\n\n            kfolds.append([x, y])\n\n        return kfolds\n\ndef get_path(id):\n    img_path = os.path.join(config.ROOT_FOLDER, 'stage' + config.STAGE + '_train', id, 'images', id + '.png')\n    target_path = os.path.join('./targets', id + '.png')\n\n    return img_path, target_path\n\ndef get_edges(img):\n    img = skimage.morphology.binary_dilation(img, selem=np.ones((5,5))).astype(np.uint8)\n    return img\n \n#\ndef get_sizes(mask_folder):\n    mask = glob.glob(os.path.join(mask_folder, 'masks/*'))[0]\n    mask = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n \n    return mask.shape\n\ndef create_masks(root_folder, stage_number, stage_section, output_folder, mode, subset=False):\n    stage_folder = os.path.join(root_folder, 'stage' + stage_number + '_' + stage_section) \n#     os.makedirs(stage_folder + '_' + mode, exist_ok=True)\n    os.makedirs('./' + mode, exist_ok=True)\n\n    if subset:\n        masks_folder = glob.glob(os.path.join(stage_folder, '*'))[:20]\n    else:\n        masks_folder = glob.glob(os.path.join(stage_folder, '*'))        \n    \n    for mask_folder in tqdm(masks_folder):\n        mask_id = mask_folder.split('/')[-1]\n\n        size = get_sizes(mask_folder)\n        masks = np.zeros(size)\n        masks_with_edges = np.zeros(size)\n\n        for mask in glob.glob(os.path.join(mask_folder, 'masks/*')):\n            ###\n            img = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n            img = img / 255.0\n\n            img_with_edges = get_edges(img)\n            \n            masks = np.add(masks, img)\n            masks_with_edges = np.add(masks_with_edges, img_with_edges)\n        \n        target = np.zeros((size[0], size[1], 3))\n        \n        target[:,:,0] = masks == 1\n        target[:,:,1] = masks_with_edges == 2\n        target[:,:,2] = masks == 0\n        \n        target *= 255\n        target = target.astype(np.uint8)\n\n        output_path = os.path.join('./' + mode, mask_id + '.png')        \n        imwrite(output_path, target)\n        \nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, pooling=True):\n        super(ConvBlock, self).__init__()\n        self.pooling = pooling\n      \n        self.conv = nn.Sequential(\n          nn.Conv2d(in_ch, out_ch, 3, padding=1),\n          nn.BatchNorm2d(out_ch),\n          nn.ReLU(inplace=True),\n          nn.Dropout2d(0.2),\n          nn.Conv2d(out_ch, out_ch, 3, padding=1),\n          nn.BatchNorm2d(out_ch),\n          nn.ReLU(inplace=True),\n          nn.Dropout2d(0.2)\n        )\n        \n    def forward(self, x):\n        x = self.conv(x)      \n  \n        if self.pooling == True:\n            x = F.max_pool2d(x, 2)\n                            \n        return x\n\nclass Upsample(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(Upsample, self).__init__()\n\n        self.upsample = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n        self.conv = ConvBlock(in_ch, out_ch, pooling=False)\n\n    def forward(self, x1, x2):\n        upsample = self.upsample(x1)\n        \n        cat = torch.cat([x2, upsample], dim=1)\n        conv = self.conv(cat)\n\n        return conv\n\nclass OutConv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(OutConv, self).__init__()\n\n        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = F.sigmoid(x)\n\n        return x\n\nclass Unet(nn.Module):\n    def __init__(self):\n        super(Unet, self).__init__()\n\n        self.in_conv = ConvBlock(3, 64, pooling=False)\n        self.down_1 = ConvBlock(64, 128)\n        self.down_2 = ConvBlock(128, 256)\n        self.down_3 = ConvBlock(256, 512)\n        self.down_4 = ConvBlock(512, 1024)\n        self.up_1 = Upsample(1024, 512)\n        self.up_2 = Upsample(512, 256)\n        self.up_3 = Upsample(256, 128)\n        self.up_4 = Upsample(128, 64)\n        \n        self.out_conv = OutConv(64, 3)\n\n    def forward(self, x):\n        x = x / 255\n    \n        x1 = self.in_conv(x)\n        x2 = self.down_1(x1)\n        x3 = self.down_2(x2)\n        x4 = self.down_3(x3)\n        x5 = self.down_4(x4)\n        x = self.up_1(x5, x4)\n        x = self.up_2(x, x3)\n        x = self.up_3(x, x2)\n        x = self.up_4(x, x1)\n        \n        outputs = self.out_conv(x)\n\n        return outputs\n\nclass Resize(object):\n    def __call__(self, sample, size):\n        img, mask = sample[0], sample[1]\n\n        img = skimage.transform.resize(img, (size, size), mode='reflect', preserve_range=True)\n        mask = skimage.transform.resize(mask, (size, size), mode='reflect', preserve_range=True)\n\n        img = img.astype(np.uint8)\n        mask = mask.astype(np.uint8)\n\n        return img, mask\n        \nclass RandomCrop(object):\n    def __call__(self, sample, size):\n        img = sample[0]\n        mask = sample[1]\n\n        h, w = img.shape[:2]\n\n        if h != size:\n            top = np.random.randint(0, h - size)\n            left = np.random.randint(0, w - size)\n        else:\n            top = 0\n            left = 0\n\n        img = img[top: top + size, left: left + size]\n        mask = mask[top: top + size, left: left + size]\n\n        return img, mask\n    \n# class CLAHE(object):\n#     def __call__(self, sample):\n#         img, mask = sample[0], sample[1]\n\n#         img = img.reshape(256, 256, 3)\n#         img = img[:,:,[2,1,0]] # flip r and b\n#         img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n#         clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(grid_size,grid_size))\n#         img[:,:,0] = clahe.apply(img[:,:,0])\n#         img = cv2.cvtColor(img, cv2.COLOR_LAB2BGR)\n#         img = img[:,:,[2,1,0]]\n#         img = img.reshape(3, 256, 256)\n        \n#         return img, mask\n        \nclass RescalingIntensity(object):\n    def __call__(self, sample):\n        img, mask = sample[0], sample[1]\n        \n        img_gray = img[:,:,0]\n        if np.mean(img_gray) > 127:\n            img = 255 - img\n    \n        return img, mask\n\nclass FlipLR(object):\n    def __call__(self, sample, p):\n        img, mask = sample[0], sample[1]\n        \n        if random.random() < p:\n            img = img[:, ::-1].copy()\n            mask = mask[:, ::-1].copy()\n            return img, mask\n        return img, mask\n\nclass FlipUD(object):\n    def __call__(self, sample, p):\n        img, mask = sample[0], sample[1]\n        \n        if random.random() < p:\n            img = img[::-1].copy()\n            mask = mask[::-1].copy()\n            return img, mask\n        return img, mask\n\nclass Rotate(object):\n    def __call__(self, sample, max_angle):\n        img, mask = sample[0], sample[1]\n\n        angle = random.randint(0, max_angle)\n        \n        img = skimage.transform.rotate(img, angle, preserve_range=True)\n        mask = skimage.transform.rotate(mask, angle, preserve_range=True)\n        \n        img = img.astype(np.uint8)\n        mask = mask.astype(np.uint8)\n\n        return img, mask\n\nclass ToTensor(object):\n    def __call__(self, sample):\n        img, mask = sample[0], sample[1]\n        \n        img = transforms.ToTensor()(img)\n        mask = transforms.ToTensor()(mask)\n        \n        return img, mask\n    \nclass CLAHE(object):\n    def __call__(self, sample):\n        img, mask = sample[0], sample[1]\n\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2Lab)\n        clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8, 8))\n        img[:, :, 0] = clahe.apply(img[:, :, 0])\n        img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n        \n        return img, mask\n\ndef augmentation(img, mask):\n    randomCrop = RandomCrop()\n    flipLR = FlipLR()\n    flipUD = FlipUD()\n    rotate = Rotate()\n    \n    img, mask = randomCrop([img, mask], config.RANDOMCROP)\n    img, mask = flipLR([img, mask], config.FLIPLR)\n    img, mask = flipUD([img, mask], config.FLIPUD)\n    img, mask = rotate([img, mask], config.ROTATE)\n\n    return img, mask\n\nclass TrainDataset(Dataset):\n    def __init__(self, ids, augmentation=None):\n        self.ids = ids\n        self.augmentation = augmentation\n        \n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        id = self.ids[idx]\n\n        img_path, targets_path = get_path(id)\n        \n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        mask = cv2.imread(targets_path, cv2.IMREAD_COLOR)\n        \n#         img, target = self.augmentation(img, target)\n        \n        clahe = CLAHE()\n        rescalingIntensity = RescalingIntensity()\n        toTensor = ToTensor()\n        randomCrop = RandomCrop()\n        \n#         img, mask = clahe([img, mask])\n        img, mask = rescalingIntensity([img, mask])\n        img, mask = rescalingIntensity([img, mask])\n        img, mask = randomCrop([img, mask], config.RANDOMCROP)\n        img, mask = toTensor([img, mask])\n\n        return {'img': img, 'target': mask}\n    \n# Taken from Heng Cher Keng's April 27 code\nclass WeightedBCELoss2d(nn.Module):\n    def __init__(self):\n        super(WeightedBCELoss2d, self).__init__()\n\n    def forward(self, logits, labels, weights):\n        w = weights.view(-1)\n        z = logits.contiguous().view(-1)\n        t = labels.contiguous().view(-1)\n\n        loss = w*z.clamp(min=0) - w*z*t + w*torch.log(1 + torch.exp(-z.abs()))\n        loss = loss.sum()/(w.sum()+ 1e-12)\n        return loss\n\ndef make_weight(labels_truth):\n    B,C,H,W = labels_truth.size()\n    weight = torch.FloatTensor(B*C*H*W).requires_grad_().cuda()\n\n    pos = labels_truth.detach().sum()\n    neg = B*C*H*W - pos\n    \n    if pos>0:\n        pos_weight = 0.5/pos\n        neg_weight = 0.5/neg\n    else:\n        pos_weight = 0\n        neg_weight = 0\n\n    weight[labels_truth.contiguous().view(-1)> 0.5] = pos_weight\n    weight[labels_truth.contiguous().view(-1)<=0.5] = neg_weight\n\n    weight = weight.view(B,C,H,W)\n    return weight\n\ndef loss(inputs, targets):\n    epsilon = 1e-5\n    inputs = torch.clamp(inputs.cpu(), epsilon, 1. - epsilon)\n    weight = 30 * targets[:,0:1].cpu() + 3 * targets[:,1:2].cpu() + 1 * targets[:,2:3].cpu()\n    \n    loss = - torch.sum(targets.cpu() * weight.cpu() * torch.log(inputs.cpu()) + (1 - targets.cpu()) * torch.log(1 - inputs.cpu()), 1)\n  \n    return loss\n\n    # mask_weights = make_weight(targets[:,0:1])\n    # edges_weights = make_weight(targets[:,1:2])\n    # backgrounds_weights = make_weight(targets[:,2:3])\n\n    # mask_loss = WeightedBCELoss2d()(inputs[:,0:1], targets[:,0:1], mask_weights)\n    # edges_loss = WeightedBCELoss2d()(inputs[:,1:2], targets[:,1:2], edges_weights)\n    # backgrounds_loss = WeightedBCELoss2d()(inputs[:,2:3], targets[:,2:3], backgrounds_weights)\n\n    # loss = 3 * dice_loss(inputs[:,0:1], targets[:,0:1]) + 30 * dice_loss(inputs[:,1:2], targets[:,1:2]) + 1 * dice_loss(inputs[:,2:3], targets[:,2:3])\n\n    return loss\n\ndef dice_loss(inputs, targets):\n    num = targets.size(0)\n    m1  = inputs.view(num,-1)\n    m2  = targets.view(num,-1)\n    intersection = (m1 * m2)\n    score = 2. * (intersection.sum(1)+1) / (m1.sum(1) + m2.sum(1)+1)\n    score = 1 - score.sum()/num\n    return score\n\ndef iou(predict, label):\n\n    # Precision helper function\n    def compute_precision(threshold, iou):\n        matches = iou > threshold\n        true_positives  = np.sum(matches, axis=1) == 1  # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    num_label   = len(np.unique(label  ))\n    num_predict = len(np.unique(predict))\n\n    # Compute intersection between all objects\n    intersection = np.histogram2d(label.flatten(), predict.flatten(), bins=(num_label, num_predict))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(label,   bins = num_label  )[0]\n    area_pred = np.histogram(predict, bins = num_predict)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred,  0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    precision = []\n    average_precision = 0\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = compute_precision(t, iou)\n        p = tp / (tp + fp + fn)\n        precision.append((t, p, tp, fp, fn))\n        average_precision += p\n\n    average_precision /= len(precision)\n    return average_precision, precision\n\ndef show_images(weights):\n    logging.info('Visualizing model')\n    model = Unet()\n    model.load_state_dict(torch.load(weights))\n    model.eval()\n\n    model.cuda()\n  \n    kfolds = get_kfolds(2)\n  \n#     dataset = TrainDataset(kfolds[0][0], x_transform=x_transforms, target_transforms=target_transforms)\n    dataset = TrainDataset(kfolds[0][0], augmentation=augmentation)\n    dataLoader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=config.SHUFFLE, num_workers=config.NUM_WORKERS)\n\n    with torch.no_grad():\n        for data in dataLoader:\n            img, target = data['img'], data['target']\n\n            x = Variable(img).cuda()\n            y = Variable(target).cuda()\n\n            outs = model(x)\n            break\n\n    y = y.detach().cpu().numpy()\n    outs = outs.detach().cpu().numpy()\n\n    fig = plt.figure(figsize=(30, 20))\n\n    ax = plt.subplot(4, 4, 1)\n    ax.set_title('Ground truth')\n    ax.imshow(y[1].reshape(256, 256, 3))\n\n    ax = plt.subplot(4, 4, 2)\n    ax.set_title('Ground truth mask')\n    ax.imshow(y[1][0].reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 3)\n    ax.set_title('Ground truth edges')\n    ax.imshow(y[1][1].reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 4)\n    ax.set_title('Ground truth background')\n    ax.imshow(y[1][2].reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 5)\n    ax.set_title('Ground truth mask - edges')\n    ax.imshow((y[1][0] - y[1][1]).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 6)\n    ax.set_title('Ground truth background - mask')\n    ax.imshow((y[1][2] - y[1][0]).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 7)\n    ax.set_title('Predicted mask')\n    ax.imshow((outs[1][0]).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 8)\n    ax.set_title('Predicted mask with Otsu thresholding')\n    ax.imshow((outs[1][0] > threshold_otsu(outs[1][0])).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 9)\n    ax.set_title('Predicted edges')\n    ax.imshow((outs[1][1]).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 10)\n    ax.set_title('Predicted edges with Otsu thresholding')\n    ax.imshow((outs[1][1] > threshold_otsu(outs[1][1])).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 11)\n    ax.set_title('Predicted background')\n    ax.imshow((outs[1][2]).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 12)\n    ax.set_title('Predicted background with Otsu thresholding')\n    ax.imshow((outs[1][2] > threshold_otsu(outs[1][2])).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 13)\n    ax.set_title('Predicted mask - edges')\n    ax.imshow((outs[1][0]) - (outs[1][1]).reshape(256, 256))  \n\n    ax = plt.subplot(4, 4, 14)\n    ax.set_title('Predicted mask - edges with Otsu thresholding')\n    ax.imshow(((outs[1][0] - outs[1][1]) > threshold_otsu((outs[1][0]) - (outs[1][1]))).reshape(256, 256))\n\n    ax = plt.subplot(4, 4, 15)\n    ax.set_title('Predicted background - mask with Otsu thresholding')\n    ax.imshow(((outs[1][2] - outs[1][0]) > threshold_otsu((outs[1][2]) - (outs[1][0]))).reshape(256, 256))","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"9555a580-047d-4adf-926d-72da9e3bdf96","_uuid":"822c2519353125180be44907c65e55c16489c44b","collapsed":true,"trusted":true},"cell_type":"code","source":"import os\nimport logging\nfrom tqdm import tqdm\nfrom glob import glob\nimport argparse\n\nimport torch\nimport torchvision\n\nfrom torch.utils.data import DataLoader\n\nimport torch.nn as nn\nfrom torch.autograd import Variable\n    \ndef subset(subset):\n    if subset == 'True':\n        logging.info('Using a subset')\n        config.SUBSET = True\n    else:\n        logging.info('Using the full dataset')\n        config.SUBSET = False\n\ndef preprocess():\n    logging.info('Starting Preprocessing')\n    logging.info('Creating targets')\n    create_masks(config.ROOT_FOLDER, config.STAGE, 'train', config.TARGETS_FOLDER, 'targets', config.SUBSET)\n\ndef train(epochs, weights, kfolds):\n    logging.info('Starting Training')\n    logging.info('Training for ' + str(epochs) + ' epochs')\n\n    kfolds = get_kfolds(kfolds)\n    logging.info(str(len(kfolds)) + ' kfolds in cross validation')\n\n    if weights != '':\n        model = Unet()\n        model = load_model(model, weights)\n\n    total_kfolds_train_loss = 0\n    total_kfolds_val_loss = 0\n\n    for i, kfold in enumerate(kfolds):\n        print('\\n')\n        logging.info('=' * 50)\n        logging.info('Kfold # ' + str(i + 1))\n\n        train_ids, val_ids = kfold[0], kfold[1]\n\n        logging.info('Creating Dataset')\n#         train = TrainDataset(train_ids, x_transform=x_transforms, target_transforms=target_transforms)\n        train = TrainDataset(train_ids, augmentation=augmentation)\n        trainDataloader = DataLoader(train, batch_size=config.BATCH_SIZE, shuffle=config.SHUFFLE, num_workers=config.NUM_WORKERS)\n        val = TrainDataset(val_ids, augmentation=augmentation)\n        valDataloader = DataLoader(val, batch_size=config.BATCH_SIZE, shuffle=config.SHUFFLE, num_workers=config.NUM_WORKERS)\n\n        if weights != '' and i == 0:\n            model = model\n            weights = ''\n        else:\n            model = Unet()\n\n        model.cuda()\n\n        optimizer = optim.Adam(model.parameters(), lr=config.LR)\n\n        early_stopping = EarlyStopping()\n\n        for epoch in range(epochs):\n            epoch += 1\n            print('\\n')\n            logging.info('-' * 50)\n            logging.info('Epoch # ' + str(epoch))\n            \n            total_train_loss = 0\n            for data in tqdm(trainDataloader):\n                img, target = data['img'], data['target']\n\n#                 x = img.requires_grad_().cuda()\n#                 y = target.requires_grad_().cuda()\n                x = Variable(img).cuda()\n                y = Variable(target).cuda()\n\n                optimizer.zero_grad()\n\n                outs = model(x)\n                train_loss = loss(outs, y)\n#                 total_train_loss += (torch.sum(train_loss.view(-1)) / len(train_loss.view(-1))).item()\n                total_train_loss += (torch.sum(train_loss.view(-1)) / len(train_loss.view(-1))).data[0]\n\n                train_loss.backward(gradient=train_loss)\n                optimizer.step()\n\n            total_val_loss = 0\n#             with torch.no_grad():\n            for data in tqdm(valDataloader):\n                img, target = data['img'], data['target']\n\n                x = Variable(img).cuda()\n                y = Variable(target).cuda()\n\n                optimizer.zero_grad()\n\n                outs = model(x)\n                val_loss = loss(outs, y)\n#                     total_val_loss += (torch.sum(val_loss.view(-1)) / len(val_loss.view(-1))).item()\n                total_val_loss += (torch.sum(val_loss.view(-1)) / len(val_loss.view(-1))).data[0]\n\n            message, train_loss, val_loss = calculate_losses(total_train_loss, total_val_loss, train_ids, val_ids, epoch)\n            print(message)\n\n            total_kfolds_train_loss += train_loss\n            total_kfolds_val_loss += val_loss\n\n            action = early_stopping.evaluate(model, val_loss, epoch, config.PATIENCE)\n\n            if action == 'save':\n                save_model(model, i)\n            elif action == 'stop':\n                break\n            else:\n                continue\n    \n    message = calculate_kfolds_losses(total_kfolds_train_loss, total_kfolds_val_loss, config.KFOLDS, config.EPOCHS)\n    print(message)\n\ndef visualize(weights):\n    show_images(weights)","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"4ddbcb48-0fd4-447a-ab2b-4eed925d2eb3","_uuid":"2115c9b481d47b5fb73237c4dc47abe0bfa86b25","collapsed":true,"trusted":true},"cell_type":"code","source":"import glob\nconfig.ROOT_FOLDER = '../input/'\nconfig.SUBSET = True","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"bfcfe20f-d846-45fd-99c1-e26f4323113d","_uuid":"2406c5ff19d5aec40d2f5bd820d9873b37e390fc","trusted":true},"cell_type":"code","source":"def get_edges(img):\n    img = skimage.morphology.binary_dilation(img, selem=np.ones((3,3))).astype(np.uint8)\n    return img\n \ndef get_sizes(mask_folder):\n    mask = glob.glob(os.path.join(mask_folder, 'masks/*'))[0]\n    mask = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n \n    return mask.shape\n\ndef create_masks(root_folder, stage_number, stage_section, output_folder, mode, subset=False):\n    stage_folder = os.path.join(root_folder, 'stage' + stage_number + '_' + stage_section) \n#     os.makedirs(stage_folder + '_' + mode, exist_ok=True)\n    os.makedirs('./' + mode, exist_ok=True)\n\n    if subset:\n        masks_folder = glob.glob(os.path.join(stage_folder, '*'))[:20]\n    else:\n        masks_folder = glob.glob(os.path.join(stage_folder, '*'))        \n    \n    for mask_folder in tqdm(masks_folder):\n        mask_id = mask_folder.split('/')[-1]\n\n        size = get_sizes(mask_folder)\n        masks = np.zeros(size)\n        masks_with_edges = np.zeros(size)\n\n        for mask in glob.glob(os.path.join(mask_folder, 'masks/*')):\n            ###\n            img = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n            img = img / 255.0\n\n            img_with_edges = get_edges(img)\n            \n            masks = np.add(masks, img)\n            masks_with_edges = np.add(masks_with_edges, img_with_edges)\n        \n        target = np.zeros((size[0], size[1], 3))\n        \n#         target[:,:,0] = masks == 1\n#         target[:,:,1] = masks_with_edges == 2\n#         target[:,:,2] = masks == 0\n        target[:,:,0] = masks_with_edges == 1\n        target[:,:,1] = masks_with_edges == 2\n        target[:,:,2] = masks_with_edges == 0\n        \n        target *= 255\n        target = target.astype(np.uint8)\n\n        return target\ntarget = create_masks(config.ROOT_FOLDER, config.STAGE, 'train', config.TARGETS_FOLDER, 'targets', config.SUBSET)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a5a87d8a97ab5be752b65fe410bae9e070a3895"},"cell_type":"code","source":"plt.imshow(target[:,:,0], cmap='gray')","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d49d417f0eb4d596eb22e347241c691a45507ad"},"cell_type":"code","source":"target[:,:,2].mean()","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"0b9af1c8-3922-4fea-a98e-369e488632c2","_uuid":"53c9b2d1b0df08c4d71f2260d8ed346f4cd62708","collapsed":true,"trusted":true},"cell_type":"code","source":"plt.imshow(target[:,:,0], cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"060e4aa9-44ae-496e-80d0-5c640ab41087","_uuid":"e4a2514d44f1bbd07aa946d7ad8cc7c2618abd6f","collapsed":true,"trusted":true},"cell_type":"code","source":"plt.imshow(target[:,:,1], cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e78ca78b-e90c-4e5f-924b-f89d5b4d1b32","_uuid":"d570f5662c5935c79e2fa7abf60de61028e885bc","collapsed":true,"trusted":true},"cell_type":"code","source":"plt.imshow(target[:,:,2], cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f3fa075b-17bb-49eb-be92-3041f1427242","_uuid":"d0bfdfdafeb8f82e5360c701dcd87c4efe724ff6","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"import glob\npreprocess()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"da793ff4-dd70-4ea7-9000-312a160e96f9","_uuid":"c3b58b13122056458275949e3c9890eebdf87217","collapsed":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"import glob\nconfig.PATIENCE = 5\nconfig.KFOLDS = 1\nconfig.BATCH_SIZE = 2\ntrain(1, '', 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9d39a590-2c1d-466a-b89d-eca02f946582","_uuid":"ac41d5d2e95076aec6021e3ad428d8323b8d70db","collapsed":true,"trusted":true},"cell_type":"code","source":"def postprocess(weights):\n    model = Unet()\n\n    model.load_state_dict(torch.load(weights, map_location=lambda storage, location: storage))\n    model.eval()\n\n    model.cuda()\n  \n    kfolds = get_kfolds(2)\n  \n    dataset = TrainDataset(kfolds[0][0], augmentation=augmentation)\n    dataLoader = DataLoader(dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS)\n\n    for data in dataLoader:\n        img, target = data['img'], data['target']\n\n        x = Variable(img).cuda()\n        y = Variable(target).cuda()\n\n        outs = model(x)\n#         break\n\n    x = x.data.cpu().numpy()\n    y = y.data.cpu().numpy()\n    outs = outs.data.cpu().numpy()\n\n    return x, y, outs\n  \n# x, y, outs = postprocess('model-45.pt')\nx, y, outs = postprocess('model-kfold-0-best.pt')\n\nfrom skimage.morphology import label, binary_dilation, erosion, binary_closing\nfrom skimage.segmentation import random_walker\n\n# outs[0][1] *= (255.0/outs[0][1].max())\n\nmask = outs[0,0]\ncontour = outs[0,1]\n    \nmask = (mask*255).astype(np.uint8)\ncontour = (contour*255).astype(np.uint8)\n\n_, mask = cv2.threshold(mask, 0, 255, cv2.THRESH_OTSU)\n_, contour = cv2.threshold(contour, 0, 255, cv2.THRESH_OTSU)\n\nsure_foreground = (contour - mask)\nsure_background = erosion(mask)\n\nmask_plus_contour = cv2.add(mask, contour)\nmask_plus_contour = cv2.cvtColor(mask_plus_contour, cv2.COLOR_GRAY2RGB)\n\nunknown = cv2.subtract(sure_background, sure_foreground)\n\n# Marker labelling\noutput = cv2.connectedComponentsWithStats(sure_foreground)\nlabels = output[1]\nstats = output[2]\n# Add one to all labels so that sure background is not 0, 0 is considered unknown by watershed\n# this way, watershed can distinguish unknown from the background\nlabels = labels + 1\nlabels[unknown==255] = 0\n\nlabels = cv2.watershed(mask_plus_contour, labels)\n# labels = random_walker(mask_plus_contour, labels, multichannel=True)   \n\nlabels[labels==-1] = 0\nlabels[labels==1] = 0\nlabels = labels -1\nlabels[labels==-1] = 0\n\nmean = np.mean(stats[1:,cv2.CC_STAT_AREA])\n\nfor i in range(1, labels.max()):\n     if stats[i, cv2.CC_STAT_AREA] > mean*10 or stats[i, cv2.CC_STAT_AREA] < mean/10:\n        labels[labels==i] = 0\n\n        \ndef renumber_labels(label_img):\n    \"\"\" Re-number nuclei in a labeled image so the nuclei numbers are unique and consecutive.\n    \"\"\"\n    new_label = 0\n    for old_label in np.unique(label_img):\n        if not old_label == new_label:\n            label_img[label_img == old_label] = new_label\n        new_label += 1\n  \n    return label_img\n\n\nlabels = renumber_labels(labels)\nimg = np.concatenate((outs[0][0:1].reshape(256, 256, 1), outs[0][1:2].reshape(256, 256, 1), (outs[0][2]).reshape(256, 256, 1)), axis=-1).reshape(256, 256, 3)\ntruth = np.concatenate((y[0][0:1].reshape(256, 256, 1), y[0][1:2].reshape(256, 256, 1), (y[0][2] > 0).astype(np.uint8).reshape(256, 256, 1)), axis=-1).reshape(256, 256, 3)\nx = np.concatenate((x[0][0:1].reshape(256, 256, 1), x[0][1:2].reshape(256, 256, 1), (x[0][2]).reshape(256, 256, 1)), axis=-1).reshape(256, 256, 3)\n\nplt.imshow(x)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c6f4c2c1-e18e-4c58-b3c6-b348323993bf","_uuid":"7e6766f861a2ecfe880d4d3610c74985e54b3d04","collapsed":true,"trusted":true},"cell_type":"code","source":"# x = x.reshape(3, 256, 256)\nx = (x*255).astype(np.uint8)\n\nimg_lab = cv2.cvtColor(x, cv2.COLOR_RGB2Lab)\nclahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8, 8))\nimg_lab[:, :, 0] = clahe.apply(img_lab[:, :, 0])\nimg_output = cv2.cvtColor(img_lab, cv2.COLOR_LAB2RGB)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8daae0de-4a7d-4875-9f0d-403199b893cb","_uuid":"40ad6a9ef413482242a79500740c8d8584864daa","collapsed":true,"trusted":true},"cell_type":"code","source":"plt.imshow(img_output)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"78c886fb-4f15-4ed7-ac18-dd529d542abf","_uuid":"3a9f54111a1911d7280b45aec71d05027d2b3a82","collapsed":true,"trusted":true},"cell_type":"code","source":"plt.imshow(truth)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0a210aec-f36f-4a65-b632-a6204557d782","_uuid":"a5e1790a871b5c549facb870c69d27a44f281bf9","collapsed":true,"trusted":true},"cell_type":"code","source":"def rgb_clahe(in_rgb_img):\n    grid_size = 8\n#     in_rgb_img = (in_rgb_img*255).astype(np.uint8)\n    bgr = in_rgb_img[:,:,[2,1,0]] # flip r and b\n    lab = cv2.cvtColor(bgr, cv2.COLOR_BGR2LAB)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(grid_size,grid_size))\n    lab[:,:,0] = clahe.apply(lab[:,:,0])\n    bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n    return bgr[:,:,[2,1,0]]\n\nplt.imshow(rgb_clahe(x))\n# x = x.reshape(256, 256, 3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"939eb072-fa61-4347-92bb-6ebe744c3188","_uuid":"cf44c7f24ef3580ec1eef637bc4174421fc266bd","collapsed":true,"trusted":true},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"47f3d5ab-6202-46db-95cf-9ad46adcc9a6","_uuid":"f61bb54304b3af879e72dfd2280760f5a8932cdf","collapsed":true,"trusted":true},"cell_type":"code","source":"plt.imshow(x.reshape(256, 256, 3))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4deec5a0-0574-4c32-8a0f-e4684f3d85d9","_uuid":"44670d5b9413b3c357f579cb9e94096809dcf01a","collapsed":true,"trusted":true},"cell_type":"code","source":"# x = x.astype(np.uint8)\nskimage.exposure.equalize_adapthist(x[:,:,0])\n# x[:,:,0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a1195d0-28a2-43b2-9f84-95de1cb0aab3","_uuid":"74f8d99653a6c41ad0b41991e3cd89fe292a419f","collapsed":true,"trusted":true},"cell_type":"code","source":"!rm -rf ./targets","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}